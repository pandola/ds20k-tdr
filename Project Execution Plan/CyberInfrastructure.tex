%---
\section{Cyberinfrastructure}
%{\it Note: Proposed Mid-scale RI-1 projects that are focused on Cyberinfrastructure should use Secitons B and C to fully describe the project.  In all cases, proposals that include the use of existing external shared cyberinfrastructure including computing, data, software and networking infrastructure and resources should discuss that utilization here}
%\cmt{Lead Editor}{Marco Rescigno}
%\cmt{Number of pages}{2-3}

%Data, hardware, and network security will be handled by the IT departments of the institutions and facilities involved in the project with support from the Collaboration to ensure the reliable storage and processing of experimental data. Software will be developed using repository-based systems with version tracking that allow for wide distribution of the code.  The Computing working group is responsible for tracking the development of collaboration software and updating the collaboration on major changes and upgrades.  They are also responsible for the implementation of the data management plan for long term storage of data and related products.  

%Option under consideration is also that of using the considerable resources available on commercial provider like Amazon or Google, which many experiments are using to run their reconstruction and analysis jobs. This option will be scrutinized in terms of cost/benefits with respect to the baseline plane outlined below when a complete event data model will be fully developed.

%---
\subsection{Cyber-Security Plan}

Data, hardware, and network security will be handled by LNGS as the host laboratory and by the facilities operating the storage and computing resources of the project, with support from the Collaboration. Computing resources will be accessed through Grid middleware components that provide services for software installation and publication, data access through a uniform security and authorization infrastructure, interfaces for remote job submission and data retrieval, and scheduling tools designed to optimize utilization of computing resources. The Grid infrastructure will be based on the infrastructure and the software tools and services developed for the LHC Computing Grid (LCG) project. 

%---
\subsection{Code Development Plan}

The \DSk\ project will use an object-oriented approach to software design, based primarily on the C++ programming language with some components implemented using other high level languages. A software framework was built during the \DSf\ experiment that met the basic processing needs of the experiment and was sufficiently flexible to meet changing requirements. A similar framework was developed for the \DEAP\ experiemnt. The collaboration is currently developing event reconstruction code for several small-scale prototype detectors, drawing from the \DSf\ and \DEAP\ experiences. In order to support code reuse, the system will be optimized to handle both the offline and software trigger environments and provide common user access to low-level algorithms used for I/O and data persistency.

%The reconstruction combines information from the \TPC\ and the veto detectors. A typical reconstruction algorithm takes one or more collections of information from the event data model (EDM) raw data stream as input, calls a set of modular tools, and outputs one or more collections of reconstructed objects. Common tools are shared between reconstruction algorithms, exploiting abstract interfaces to reduce dependencies.  Analysis of calibration data will also be performed within the reconstruction and simulation software environment. 

The computing group will provide and maintain the software development environment. This includes supporting code management tools (Git, Github, etc.), defining the use of external software (for example CERN Root, \Geant, theory interpretation codes, etc.), providing scripting for building software releases, producing code distribution kits, and providing documentation such as web, wiki pages, and bug reporting. The computing group plans to make use of standard quality assurance and quality control tools used in HEP large experiments. Doxygen and Twiki web pages will be used to document code, version the software for specific sub-projects, and provide instructions and tutorials. Production releases will be used to define the stable code used for production and end-user analysis. Production release builds will be managed and supervised by a team of librarians within the computing group. The librarians will ensure tall software components work coherently and will patch production releases with bug-fixes when needed. Production releases will be installed on the Grid using the methods developed for LCG.

%---
\subsection{Data Management Plan}

The data collected by the \DSks\ experiment will probe novel parameter space for \WIMP\ dark matter, and as such, the results of the experiment will be of interest to the particle physics, astrophysics, and astronomy communities. The \GADMC\ will engage with its host Laboratory to develop protocols for data archiving, retrieval, security, and dissemination as detailed below.  Routine data backups will be performed, with at least one backup off-site. All researchers will be obliged to adhere to the data management plan as a requirement for data usage.

{\bf Expected Data:} Data will consist of records that represent the response of photodetectors to light signals generated by particle interactions in the detector. The raw data will contain the necessary information to reconstruct each photodetector's response as a function of time along with metadata stored in headers. Accompanying the raw data will be electronic log files and an indexed laboratory notebook narrative that can be cross-referenced with the raw events.  Raw data will be stored on disk and immediately processed to monitor the data in real time.  Once processed, the events will be stored in an open source documented format.  Both raw data files and processed event files will be preserved off-site at the National Center for Frame Analysis (CNAF) copies on LTO tapes and RAID arrays.  Copies of the electronic logbook and the original laboratory notebooks will be preserved and secured at the storage facilities of collaborating institutions.

{\bf Data Processing:} Primary event processing will occur at the experimental site in the software trigger farm. Pre-processed data is archived on temporary storage at the experimental site then copied to central computing centers (Tier-1/Tier-2). These facilities archive the pre-processed data, provide reprocessing capacity, and allow analysis of the processed data. Derived datasets produced in the physics analyses will also be copied to the Tier-1/Tier-2 facilities for further analysis and long-term storage. The Tier-1/Tier-2 facilities will also provide the processing capacity for detector simulations. The safety of archived raw data will be assured by creating multiple copies of data, either on tape and disk or by distributing copies across several data centers in Europe and America. Bulk data processing is expected to be performed using low cost commodity cluster computing based on commercial \CPUs.  Final data analysis will be performed either directly at the Tier-1/Tier-2 centers or on commercial \CPUs\ hosted at institutes participating in the experiment.  

{\bf Data Storage:} Currently, the amount of short term storage currently available at \LNGS\ for \DSf\ consists of \DSfDataStorageLNGSShortDisk\ of front-end storage used as a temporary buffer and located in the underground laboratory plus \DSfDataStorageLNGSLongDisk\ of disk space in the above ground computing center for short- and long-term storage of \DSf\ data.  From there, raw data are copied to \CNAF\ and Fermilab for reprocessing and analysis.  \CNAF\ provides  \DSfDataStorageCNAFDisk\ of disk storage and \DSfDataStorageCNAFTape\ of tape storage.  At Fermilab, there is \DSfDataStorageFNALDisk\ of fault-tolerant disk storage and about \DSfDataStorageFNALTape\ on the dCache-based tape system for long-term storage.  It is expected that that a factor of twenty more resources will be needed for a ten year run of \DSks.  The total amount of storage for the ten years of data-taking, including physics, calibration, and simulated data, will be \DSkDataStorageTotalDiskNewProposalExtended\ of disk storage and \DSkDataStorageTotalDiskNewProposalExtended\ of tape storage. The processing power currently used for reprocessing and analysis of \DSf\ data includes a farm of \DSfCPUsLNGS\ at \LNGS\ for production and validation, plus \DSfCPUsCNAF\ at \CNAF\ and  \DSfCPUsFNALUpgrade on the Fermilab grid system.  A factor of twenty increase is also needed for the CPU resources assuming only a modest increase in the analysis code execution time is required given the highly compressed data format. Assuming a factor two increase in the CPU time needed to reconstruct an event and a factor ten to simulate a full event, \DSks\ needs in total about \DSkOffLineCoresTotalNewProposal\ dedicated cores to maintain reconstruction in realtime of the collected data and to produce simulated samples.

{\bf Period of Data Retention:} The data will be preserved for at least three years beyond the award period, as required by NSF guidelines.  All data will be held on tape for at least ten years.  One month before deletion of any data, the collaboration will be informed that the data will be removed so that there is sufficient time to request an extension and/or download the files.  A second warning will be sent two weeks before the action and a final warning will be distributed in the last week.

{\bf Data Formats and dissemination:} All significant findings stemming from the research program will be promptly prepared and submitted for publication with authorship that accurately reflects the contributions of those involved as decided by the IB.  Data will also be used to produce posters, talks, papers, and informative reports for both expert and non-expert audiences.  After publications are generated and accepted by the relevant journals and thesis works are published, data generated from this project may be released upon request to non-collaboration members, at the discretion of the \GADMC, provided that the release does not compromise the ability of the Collaboration to perform and publish additional analyses.  We do not anticipate that there will be any significant intellectual property issues involved with the acquisition, processing, or analysis of the data.  In the event that discoveries or inventions are made in direct connection with this data, access to the data will be granted upon request once appropriate invention disclosures and/or provisional patent filings are made.  Access to the data will be provided via written request to the Spokesperson.   \\

This project will not involve the acquisition of either animal or human subjects data.






