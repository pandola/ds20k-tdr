\subsection{Computing}
\label{sec:Computing}

The data storage and offline processing system must support transfer, storage, and analysis of the data recorded by the \DAQ\ system, for the entire lifespan of the experiment.  It must provide for production and distribution of simulated data, access to conditions and calibration information and other non-event data, and provide resources for the physics analysis activities of the collaboration. Necessary components for data storage and offline systems are the software framework and services, as well as the data management system, user-support services, and the world-wide data access and analysis job-submission system.  The design of the system is built upon the knowledge acquired in the construction and operations of the \DSfs\ detector. 

The large number of channels in the \LArTPC\ makes it impractical to digitize and save the full waveform of each channel, as done in \DSfs.  Nevertheless, the charge and hit time information that will be saved will preserve all the necessary details about the amplitude and time evolution of the signals generated in the target. With appropriate filtering and compression, in addition to the expected background reduction described elsewhere in this document, the amount of data selected for recording in \DSks\ is expected to be only a few times that of \DSfs.  The \DSfs\ experiment typically collects about \DSfDataTotalRate, of which \DSfDataLaserRate\ come from laser calibrations, with the remaining \DSfDataDMSRate\ from the dark matter search.  The size of a \DSfs\ laser calibration event is about \DSfDataLaserSize, while the size of a dark matter search data event is about \DSfDataDMSSize.  This is expected to decrease by a factor of \DSkDSfDataReductionFactor\ in \DSks, despite the much larger number of channels.  Taking into account the event size and data rate, the improved background rejection and data filtering, and using the experience from \DSfs, the short-term storage required at the experimental site is expected to be \DSkDataStorageLNGSShortDisk.  The total storage inventory required for the experiment is expected to be more than \DSkDataStorageTotalDiskNewProposalExtended, including the storage needed for simulated and reconstructed events. 


%---
\subsubsection{Computing systems and data workflow}
The primary event processing will occur at the experimental site in the software trigger farm. Pre-processed data is archived on the temporary storage at the experimental site and copied to central computing centers (Tier-1/Tier-2). These facilities archive the pre-processed data, provide the reprocessing capacity, provide access to the various processed versions, and allow analysis of the processed data. Derived datasets produced in the physics analyses are also copied to the Tier-1/Tier-2 facilities for further analysis and long-term storage. The Tier-1/Tier-2 facilities also provide the simulation capacity for the \DSks\ experiment. 

Bulk data processing is expected to be performed using low cost commodity cluster computing based on commercial \CPUs.  Final data analysis will be performed either directly at the Tier-1/Tier-2 centers or on commercial \CPUs\ hosted at institutes participating in the experiment.  There is also the option of using the considerable free resources available on the Open Source Grid, which many experiments are currently using to run their reconstruction and analysis jobs.

The amount of short term storage currently available at \LNGS\ for \DSfs\ consist of \DSfDataStorageLNGSShortDisk\ of front-end storage used as temporary buffer and located in the underground laboratory, plus \DSfDataStorageLNGSLongDisk\ of disk space in the above ground computing center for short- and long-term storage of \DSfs\ data.  From there, raw data are copied to \CNAF\ and Fermilab for reprocessing and analysis.  \CNAF\ is making available \DSfDataStorageCNAFDisk\ of disk storage and \DSfDataStorageCNAFTape\ of tape storage.  At Fermilab, there is \DSfDataStorageFNALDisk\ of fault-tolerant disk storage and about \DSfDataStorageFNALTape\ on the dCache-based tape system for long-term storage.  It is expected that much of this inventory will be recycled for the \DSks\ experiment, aside from what is necessary for ongoing storage of \DSfs\ raw data.  Any necessary additional tape storage will be purchased and installed. The total amount of storage for the ten years of data-taking, including calibration and simulated data, is \DSkDataStorageTotalDiskNewProposalExtended\ of disk storage and \DSkDataStorageTotalDiskNewProposalExtended\ of tape storage. The processing power currently used for reprocessing and analysis of \DSf\ data includes a farm of \DSfCPUsLNGS\ at \LNGS\ for production and validation, plus \DSfCPUsCNAF\ at \CNAF\ and \DSfCPUsFNAL\ (soon to increase to \DSfCPUsFNALUpgrade) on the Fermilab grid system.  

Currently, a single \DSfs\ event takes about a half of a second to reconstruct on a typical \SI{2.8}{\giga\hertz} processor, meaning that \DSkOffLineCoresRealtimeReconNumber\ dedicated cores can maintain reconstruction in realtime for an event rate up to \DSkComputingRealtimeMaxEventRate.  Assuming a factor two increase in the cpu time needed to reconstruct an event,  a factor ten to simulate a full event, a real data plus calibration event rate of  \DSkRealtimeCalibrationEventRate, and a sample of simulated events of the same dimension of the real-data one, \DSks\ needs \DSkOffLineCoresRealtimeReconNumber\ dedicated cores to maintain reconstruction in realtime of the collected real data + calibration events, and \DSkOffLineCoresSimulationNumber\ dedicated cores to produce the simulated samples. Moreover, \DSkOffLineCoresReprocessOneYearNumber\ physical cores are sufficient to reprocess all physics events collected in one year in a three month time period. 

%---
\subsubsection{Software Environment}
\DSks\ will adopt an object-oriented approach to software, based primarily on the C++ programming language,  with some components implemented using other high level languages (Python etc.). A software framework has been built up during the \DSfs\ experiment which provides flexibility in meeting the basic processing needs of the experiment, as well as in  responding to changing requirements. In order to support code reuse, build a system optimized for both the offline and software trigger environment, and provide common user access to low-level algorithms used for I/O and data persistency, the C++ code will make heavy use of object oriented abstract interfaces techniques.

The reconstruction combines information from the \TPC\ and the veto detectors. A typical reconstruction algorithm takes one or more collections of information from the event data model (EDM) raw data stream as input, calls a set of modular tools, and outputs one or more collections of reconstructed objects. Common tools are shared between reconstruction algorithms, exploiting abstract interfaces to reduce dependencies.  Analysis of calibration data will also be performed within the reconstruction and simulation software environment. 

\DSks\ will produce roughly \DSkAnnualDataProduction\ of data annually, combining the data processing, simulation, calibration, and distributed analysis activities.  A data storage and management infrastructure is necessary to allow efficient storage and access to all this data.  Two types of data storage are foreseen.  The first is a file-based data and the second is relational-database-resident data.  File storage is used for bulky items such as event data (physics data, calibration data and simulation data). Database storage is used for other types of information, including technical data like detector production, installation, survey and geometry data, online/TDAQ databases, conditions databases (online and offline), offline processing configuration and bookkeeping information, and to support distributed data and database management services. File-based storage of C++ objects will be implemented through the use of ROOT I/O, which provides high performance and highly scalable object serialization to random-access files. Database storage will be based on SQL-based relational databases (MySQL and SQLite). All these technologies are widely used and well tested in HEP experiments. 

Computing resources will be accessed through Grid middleware components and services.  These provide services for software installation and publication, production operations, and data access for analysis through a uniform security and authorization infrastructure, as well as interfaces for remote job submission and data retrieval, and job scheduling tools designed to optimize utilization of computing resources. The Grid infrastructure will be based on the infrastructure and the software tools and services developed for the LHC Computing Grid (LCG) project.

The High-Level Software Trigger (HLST) provides the online event selection. The  trigger is based on an online version of the \DSfs\ reconstruction software, which has been optimized and tailored for the \DSks\ online environment running on farms of Linux PCs and/or GPUs farms.  Overall, the HLST has to provide the required rate reduction and pre-processing of raw data, including the production of global event records in analysis EDM format. The HLST will use the offline computing environment, allowing \DSks\ considerable commonality in the design and management of the selection software itself.  This also allows the HLST to use various offline software components, like detector description, calibrations, EDM, and reconstruction algorithms.  The same infrastructure can be used for data monitoring by simply replacing or augmenting the selection algorithms with those for monitoring.

%---
\subsubsection{Simulation}
\GFDS\ is a \Geant-based simulation toolkit specifically developed for \DS. The modular architecture of the code was developed in order to describe the energy and time responses of all the detectors belonging to the \DS\ program, namely \DSts, \DSfs, and \DSks, but other variations of the geometries as well. For each of them, \GFDS\ provides a rich set of particle generators, detailed geometries, real data tuned physical processes, and the full optical propagation of the photons produced by scintillation in liquid argon and by electroluminescence in gaseous argon.  The main goals of \GFDS\ are to accurately describe the light response, calibrate the energy responses in \SOne\ and \STwo\ and the time response expressed by the \FNine\ variable, tuning of the analysis cuts and their efficiency estimation, prediction of the electron and nuclear recoil backgrounds, and definition of the signal acceptance band.  

\GFDS\ tracks photons up to their conversion to photoelectrons, which occurs when a photon reaches the active region of a photosensor and survives according to the quantum efficiency. The conversion of the photoelectron into a charge signal is handled by the electronic simulation, an independent, custom-made C++ code. The electronic simulation embeds all the effects induced by the photosensors (e.g. after-pulse and cross talk) and by the electronics itself (e.g. saturation). The electronic simulation also has the option to overlap simulated events and real data baselines in order to provide more realistic simulations.  As output, it produces waveforms for each channel with the same data format of the real data, in order to be processed by the same reconstruction code. 

\GFDS\ can also track events generated by \FLUKA\ and \TALYS\ simulation codes, previously developed within the collaboration.  The \FLUKA\ simulation is mostly used to study cosmogenic neutrons and isotope productions, while \TALYS\ is used for (alpha,n) reactions, both providing input for the prediction of the nuclear recoil background. 